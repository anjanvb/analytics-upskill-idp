{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Document Processing\n",
    "\n",
    "Documents contain valuable information and come in various shapes and forms. In most cases, you are manually processing these documents which is time consuming, prone to error, and costly. Not only do you want this information extracted quickly but can also automate business processes that presently relies on manual inputs and intervention across various file types and formats.\n",
    "\n",
    "To help you overcome these challenges, AWS Machine Learning (ML) now provides you choices when it comes to extracting information from complex content in any document format such as insurance claims, mortgages, healthcare claims, contracts, and legal contracts.\n",
    "\n",
    "The diagram below shows an architecture for an Intelligent document processing workflow. It starts with data capture stage to securely store and aggregate different types (PDF, PNG, JPEG, and TIFF), formats, and layouts of documents. Followed by accurate classification of documents and extracting text and key insights from documents and perform further enrichments of the documents (such as identity entities, redaction etc.). Finally, the verification and review stage involves manual review of the documents for quality and accuracy, followed by consumption of the documents and extracted information into downstream databases/applications.\n",
    "\n",
    "In this workshop, we will explore the various aspects of this workflow such as the document classification, text and insights extraction, enrichments, and human review.\n",
    "\n",
    "![Arch](./images/idp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "In this lab we will walk you through an hands-on lab on document classification using Amazon Comprehend\n",
    "Custom Classifier. We will use Amazon Textract to first extract the text out of our documents and then label them and then use the data for training our Amazon comprehend custom classifier. We will create an Amazon Comprehend real time endpoint with the custom classifier to classify our documents.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/Insurance_doc_classify.png\" alt=\"IDP Classify\" width=\"900px\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "- [Step 1: Create Amazon Comprehend Classification Training Job](#step1)\n",
    "- [Step 2: Create Amazon Comprehend real time endpoint](#step2)\n",
    "- [Step 3: Classify Documents using the real-time endpoint](#step3)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create Amazon Comprehend Classification Training Job <a id=\"step1\"></a>\n",
    "\n",
    "In this step, we will import some necessary libraries that will be used throughout this notebook.\n",
    "\n",
    "We will then use a prepared dataset, of the appropriate filetype (.csv) and structure - one column containing the raw text of a document, and the other column containing the label of that document.\n",
    "\n",
    "Please see this [notebook](https://github.com/aws-samples/aws-ai-intelligent-document-processing/blob/main/industry/mortgage/01-document-classification.ipynb) from our mortgage blog series for detailed steps on data preparation for ingestion into Amazon Comprehend for [Amazon Comprehend custom classification model](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html). Note: While the documents may be different, the process, from a code perspective, is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: amazon-textract-response-parser in /opt/conda/lib/python3.7/site-packages (0.1.33)\n",
      "Requirement already satisfied: marshmallow==3.14.1 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-response-parser) (3.14.1)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-response-parser) (1.24.62)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.62 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-response-parser) (1.27.62)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-response-parser) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-response-parser) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.62->boto3->amazon-textract-response-parser) (1.26.12)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.62->boto3->amazon-textract-response-parser) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.62->boto3->amazon-textract-response-parser) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: amazon-textract-caller in /opt/conda/lib/python3.7/site-packages (0.0.25)\n",
      "Requirement already satisfied: amazon-textract-response-parser>=0.1.27 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-caller) (0.1.33)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.7/site-packages (from amazon-textract-caller) (1.27.62)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-caller) (1.24.62)\n",
      "Requirement already satisfied: marshmallow==3.14.1 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-response-parser>=0.1.27->amazon-textract-caller) (3.14.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-caller) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-caller) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore->amazon-textract-caller) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore->amazon-textract-caller) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->amazon-textract-caller) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: amazon-textract-prettyprinter in /opt/conda/lib/python3.7/site-packages (0.0.16)\n",
      "Requirement already satisfied: tabulate==0.8.10 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-prettyprinter) (0.8.10)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.7/site-packages (from amazon-textract-prettyprinter) (1.27.62)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-prettyprinter) (1.24.62)\n",
      "Requirement already satisfied: amazon-textract-response-parser>=0.1.27 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-prettyprinter) (0.1.33)\n",
      "Requirement already satisfied: marshmallow==3.14.1 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-response-parser>=0.1.27->amazon-textract-prettyprinter) (3.14.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-prettyprinter) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-prettyprinter) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore->amazon-textract-prettyprinter) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore->amazon-textract-prettyprinter) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->amazon-textract-prettyprinter) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install amazon-textract-response-parser --upgrade\n",
    "!python -m pip install amazon-textract-caller --upgrade\n",
    "!python -m pip install amazon-textract-prettyprinter --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from textractprettyprinter.t_pretty_print import Textract_Pretty_Print, get_string\n",
    "from trp import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the import statements above fails then please restart the notebook kernel by clicking the circular arrow button at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role is: arn:aws:iam::710096454740:role/service-role/AmazonSageMaker-ExecutionRole-20220504T135260\n",
      "Default SageMaker Bucket: s3://sagemaker-us-east-1-710096454740\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "from IPython.display import Image, display, HTML, JSON\n",
    "\n",
    "# variables\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "os.environ[\"BUCKET\"] = data_bucket\n",
    "os.environ[\"REGION\"] = region\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker role is: {role}\\nDefault SageMaker Bucket: s3://{data_bucket}\")\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "textract = boto3.client('textract', region_name=region)\n",
    "comprehend=boto3.client('comprehend', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the pre-prepared dataset and upload it to Amazon S3. The dataset is in `CSV` format and will be named `comprehend_train_data.csv`. Note that you can have more than one `CSV` file in an S3 bucket for training a Comprehend custom classifier. If you have more than one file, you can specify only the bucket/prefix in call to train the custom classifier. Amazon Comprehend will automatically use all the files under the bucket/prefix for training purposes.\n",
    "\n",
    "The following code cells will upload the training data to the S3 bucket, and create a Custom Comprehend Classifier. You can also create a custom classifier manually, please see the subsequent sections for instructions on how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Comprehend training data to S3\n",
    "key='idp/comprehend/comprehend_train_data.csv'\n",
    "s3.upload_file(Filename='./dataset/comprehend_train_data.csv', \n",
    "               Bucket=data_bucket, \n",
    "               Key=key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Once we have a labeled dataset ready we are going to create and train a [Amazon Comprehend custom classification model](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html) with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Amazon Comprehend custom classification Training Job\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> <b style=\"font-size: 24px\">üí° NOTE:</b> <p style=\"font-size: 20px\">Executing the model training code block below will start a training job which can take upwards of 40 to 60 minutes to complete. In order to save time, you can skip to the \"Import and existing classification model\" section to import a pre-trained Comprehend classifier model</p> </div>\n",
    "\n",
    "We will use Amazon Comprehend's Custom Classification to train our own model for classifying the documents. We will use Amazon Comprehend `CreateDocumentClassifier` API to create a classifier which will train a custom model using the labeled data CSV file we created above. The training data contains extracted text, that was extracted using Amazon Textract, and then labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehend Custom Classifier created with ARN: arn:aws:comprehend:us-east-1:710096454740:document-classifier/insurance-doc-classifier-idp/version/v1\n"
     ]
    }
   ],
   "source": [
    "# Create a document classifier\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "id = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "\n",
    "document_classifier_name = 'insurance-doc-classifier-idp'\n",
    "document_classifier_version = 'v1'\n",
    "document_classifier_arn = ''\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    create_response = comprehend.create_document_classifier(\n",
    "        InputDataConfig={\n",
    "            'DataFormat': 'COMPREHEND_CSV',\n",
    "            'S3Uri': f's3://{data_bucket}/{key}'\n",
    "        },\n",
    "        DataAccessRoleArn=role,\n",
    "        DocumentClassifierName=document_classifier_name,\n",
    "        VersionName=document_classifier_version,\n",
    "        LanguageCode='en',\n",
    "        Mode='MULTI_CLASS'\n",
    "    )\n",
    "    \n",
    "    document_classifier_arn = create_response['DocumentClassifierArn']\n",
    "    \n",
    "    print(f\"Comprehend Custom Classifier created with ARN: {document_classifier_arn}\")\n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'A classifier with the name \"{document_classifier_name}\" already exists.')\n",
    "        document_classifier_arn = f'arn:aws:comprehend:{region}:{account_id}:document-classifier/{document_classifier_name}/version/{document_classifier_version}'\n",
    "        print(f'The classifier ARN is: \"{document_classifier_arn}\"')\n",
    "    else:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'document_classifier_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "%store document_classifier_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check status of the Comprehend Custom Classification Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:22:58 : Custom document classifier: TRAINED\n",
      "CPU times: user 1.61 s, sys: 191 ms, total: 1.8 s\n",
      "Wall time: 1h 45min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "jobArn = create_response['DocumentClassifierArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_custom_classifier = comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_classifier[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier: {status}\")\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, to create a Comprehend Custom Classifier Job manually using the console go to [Amazon Comprehend Console](https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#classification)\n",
    "  \n",
    "- On the left menu click \"Custom Classification\"\n",
    "- In the \"Classifier models\" section, click on \"Create new model\"\n",
    "- In Model Setting for Model name, enter a name \n",
    "- In Data Specification; select \"Using Single-label\" mode and for Data format select CSV file\n",
    "- For Training dataset browse to your data-bucket created above and select the file `comprehend_train_data.csv`\n",
    "- For IAM role select \"Create an IAM role\" and specify a prefix (this will create a new IAM Role for Comprehend)\n",
    "- Click create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job can take ~30 minutes to complete. Once the training job is completed move on to next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import an existing classification model\n",
    "\n",
    "You can import a trained classification model from a different AWS account using Amazon Comprehend `ModelImport`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_response = comprehend.import_model(\n",
    "    SourceModelArn='arn:aws:comprehend:us-east-1:710096454740:document-classifier/insurance-doc-classifier-idp/version/v1',\n",
    "    ModelName='insurance-doc-classifier-idp',\n",
    "    VersionName='v1'\n",
    ")\n",
    "document_classifier_arn = import_response['ModelArn']\n",
    "document_classifier_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Classify Documents using the custom classifier <a id=\"step3\"></a>\n",
    "\n",
    "In this final step we will use the Comprehend classifier model that we just trained/imported to classify a group of un-identified documents. We will use Comprehend [StartDocumentClassificationJob](https://docs.aws.amazon.com/comprehend/latest/APIReference/API_StartDocumentClassificationJob.html) API to run an asynchronous job that will classify our documents. Note that an asynchronous classification job is capable of reading PDF, JPG, PNG and TIFF files since it can use Amazon Textract behind the scenes to extract the text from the documents.\n",
    "\n",
    "Amazon Comprehend Async classification currently only works with UTF-8 encoded plaintext files only. So we will extract the text out of our sample documents with textract and upload them to S3 first. We will use `ONE_DOC_PER_FILE` mode which signifies that each plaintext file is a single document (the other mode is `ONE_DOC_PER_LINE` which means every line in the plaintext file is a document, this is best suited for small documents such as product reviews or customer service chat transcripts etc.). More on this, see [documentation](https://docs.aws.amazon.com/comprehend/latest/dg/how-class-run.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: dataset/document_samples/txt/CMS1500.png.txt to s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/txt/CMS1500.png.txt\n",
      "upload: dataset/document_samples/txt/dr-note-sample.png.txt to s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/txt/dr-note-sample.png.txt\n",
      "upload: dataset/document_samples/txt/drivers_license.png.txt to s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/txt/drivers_license.png.txt\n",
      "upload: dataset/document_samples/txt/insurance_invoice.png.txt to s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/txt/insurance_invoice.png.txt\n",
      "upload: dataset/document_samples/txt/discharge-summary.png.txt to s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/txt/discharge-summary.png.txt\n",
      "upload: dataset/document_samples/txt/insurance_card.png.txt to s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/txt/insurance_card.png.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert images to .txt\n",
    "import os\n",
    "import shutil\n",
    "from textractcaller.t_call import call_textract\n",
    "from textractprettyprinter.t_pretty_print import Textract_Pretty_Print, get_string\n",
    "\n",
    "listfiles = os.listdir('./dataset/document_samples')\n",
    "os.makedirs('./dataset/document_samples/txt', exist_ok=True)\n",
    "\n",
    "for imagefile in listfiles:\n",
    "    if imagefile != '.ipynb_checkpoints':\n",
    "        # using amazon-textract-caller to call Textract DetectDocumntText\n",
    "        response = call_textract(input_document=f'./dataset/document_samples/{imagefile}') \n",
    "        # using pretty printer to get all the lines int the document\n",
    "        lines = get_string(textract_json=response, output_type=[Textract_Pretty_Print.LINES])\n",
    "        print(f\"Writing plaintext file for {imagefile}\")\n",
    "        with open(f'./dataset/document_samples/txt/{imagefile}.txt', \"w\") as text_file:\n",
    "            text_file.write(lines)\n",
    "        \n",
    "\n",
    "!rm -rf ./dataset/document_samples/txt/.ipynb_checkpoints\n",
    "!aws s3 sync ./dataset/document_samples/txt s3://{data_bucket}/idp/comprehend/sampledocs/txt\n",
    "    \n",
    "#delete converted files\n",
    "shutil.rmtree('./dataset/document_samples/txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the async Comprehend classification job\n",
    "\n",
    "Using the `StartDocumentClassificationJob` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Comprehend Classification job insurance-classification-job-1664815530.9945142...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'JobId': 'c8b7b3b71d8cf04d35c7f510a2f17513',\n",
       " 'JobArn': 'arn:aws:comprehend:us-east-1:710096454740:document-classification-job/c8b7b3b71d8cf04d35c7f510a2f17513',\n",
       " 'JobStatus': 'SUBMITTED',\n",
       " 'ResponseMetadata': {'RequestId': 'c776f7f6-abf0-4639-bf7d-6c010be2bc4e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c776f7f6-abf0-4639-bf7d-6c010be2bc4e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '182',\n",
       "   'date': 'Mon, 03 Oct 2022 16:45:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "jobname = f'insurance-classification-job-{time.time()}'\n",
    "print(f'Starting Comprehend Classification job {jobname} with model {document_classifier_arn}')\n",
    "\n",
    "classify_response = comprehend.start_document_classification_job(\n",
    "    JobName=jobname,\n",
    "    DocumentClassifierArn=document_classifier_arn,\n",
    "    InputDataConfig={\n",
    "        'S3Uri': f's3://{data_bucket}/idp/comprehend/sampledocs/txt',\n",
    "        'InputFormat': 'ONE_DOC_PER_FILE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': f's3://{data_bucket}/idp/comprehend/sampledocs/classifieroutput'\n",
    "    },\n",
    "    DataAccessRoleArn=role\n",
    ")\n",
    "classify_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check status of the classification job\n",
    "\n",
    "If the job completes then download the output predictions file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:37:03 : Custom document classifier Job: COMPLETED\n",
      "Output generated - s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/classifieroutput/710096454740-CLN-c8b7b3b71d8cf04d35c7f510a2f17513/output/output.tar.gz\n",
      "download: s3://sagemaker-us-east-1-710096454740/idp/comprehend/sampledocs/classifieroutput/710096454740-CLN-c8b7b3b71d8cf04d35c7f510a2f17513/output/output.tar.gz to classifyoutput/output.tar.gz\n",
      "CPU times: user 40.2 ms, sys: 16.7 ms, total: 56.9 ms\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_job = comprehend.describe_document_classification_job(\n",
    "        JobId=classify_response['JobId']\n",
    "    )\n",
    "    status = describe_job[\"DocumentClassificationJobProperties\"][\"JobStatus\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier Job: {status}\")\n",
    "    \n",
    "    if status == \"COMPLETED\" or status == \"FAILED\":\n",
    "        if status == \"COMPLETED\":\n",
    "            classify_output_file = describe_job[\"DocumentClassificationJobProperties\"][\"OutputDataConfig\"][\"S3Uri\"]\n",
    "            print(f'Output generated - {classify_output_file}')\n",
    "            !mkdir -p classifyoutput\n",
    "            !aws s3 cp {classify_output_file} ./classifyoutput\n",
    "            \n",
    "            opfile = os.path.basename(classify_output_file)\n",
    "            # open file\n",
    "            file = tarfile.open(f'./classifyoutput/{opfile}')\n",
    "            # extracting file\n",
    "            file.extractall('./classifyoutput')\n",
    "            file.close()\n",
    "            \n",
    "            with open('./classifyoutput/predictions.jsonl') as f:\n",
    "                classification_predictions = f.readlines()\n",
    "        else:\n",
    "            print(\"Classification job failed\")\n",
    "            print(describe_job)\n",
    "        break\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the classification output file\n",
    "\n",
    "The output file is a file containing json lines. The inference output contains the name of the file and the classes and respective score for each class. Highest score of the the class is the class that document belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: dr-note-sample.png.txt\n",
      "\t - Class: DISCHARGE_SUMMARY, Score: 0.9986\n",
      "\t - Class: MEDICAL_TRANSCRIPTION, Score: 0.0008\n",
      "\t - Class: INSURANCE_ID, Score: 0.0002\n",
      "\n",
      "\n",
      "File: insurance_card.png.txt\n",
      "\t - Class: INSURANCE_ID, Score: 0.9958\n",
      "\t - Class: CMS1500, Score: 0.0011\n",
      "\t - Class: LICENSE, Score: 0.001\n",
      "\n",
      "\n",
      "File: CMS1500.png.txt\n",
      "\t - Class: CMS1500, Score: 0.991\n",
      "\t - Class: PASSPORT, Score: 0.0026\n",
      "\t - Class: INSURANCE_ID, Score: 0.0025\n",
      "\n",
      "\n",
      "File: drivers_license.png.txt\n",
      "\t - Class: LICENSE, Score: 0.9969\n",
      "\t - Class: INSURANCE_ID, Score: 0.0008\n",
      "\t - Class: CMS1500, Score: 0.0006\n",
      "\n",
      "\n",
      "File: discharge-summary.png.txt\n",
      "\t - Class: DISCHARGE_SUMMARY, Score: 0.9958\n",
      "\t - Class: INSURANCE_ID, Score: 0.0033\n",
      "\t - Class: INVOICE_RECEIPT, Score: 0.0003\n",
      "\n",
      "\n",
      "File: insurance_invoice.png.txt\n",
      "\t - Class: INVOICE_RECEIPT, Score: 0.9997\n",
      "\t - Class: INSURANCE_ID, Score: 0.0001\n",
      "\t - Class: CMS1500, Score: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for predictions in classification_predictions:\n",
    "    pred = json.loads(predictions)\n",
    "    print(f\"File: {pred['File']}\")\n",
    "    for classification in pred['Classes']:\n",
    "        print(f\"\\t - Class: {classification['Name']}, Score: {classification['Score']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2 (_optional_): Create Amazon Comprehend real time endpoint <a id=\"step2\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>‚ö†Ô∏è Note:</b> Creation of a real-time endpoint can take up to 15 minutes.\n",
    "</div>\n",
    "\n",
    "\n",
    "Once our Comprehend custom classifier is fully trained (i.e. status = `TRAINED`). You can also create a real-time endpoint. You can then use this endpoint to classify documents in real time. The following code cells use the `comprehend` Boto3 client to create an endpoint, but you can also create one manually via the console. Instructions on how to do that can be found in the subsequent section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created with ARN: arn:aws:comprehend:us-east-1:710096454740:document-classifier-endpoint/insurance-custom-classifier-endpoint\n"
     ]
    }
   ],
   "source": [
    "#create comprehend endpoint\n",
    "model_arn = document_classifier_arn\n",
    "ep_name = 'insurance-custom-classifier-endpoint'\n",
    "\n",
    "try:\n",
    "    endpoint_response = comprehend.create_endpoint(\n",
    "        EndpointName=ep_name,\n",
    "        ModelArn=model_arn,\n",
    "        DesiredInferenceUnits=1,    \n",
    "        DataAccessRoleArn=role\n",
    "    )\n",
    "    ENDPOINT_ARN=endpoint_response['EndpointArn']\n",
    "    print(f'Endpoint created with ARN: {ENDPOINT_ARN}')    \n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'An endpoint with the name \"{ep_name}\" already exists.')\n",
    "        ENDPOINT_ARN = f'arn:aws:comprehend:{region}:{account_id}:document-classifier-endpoint/{ep_name}'\n",
    "        print(f'The classifier endpoint ARN is: \"{ENDPOINT_ARN}\"')\n",
    "        %store ENDPOINT_ARN\n",
    "    else:\n",
    "        print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ENDPOINT_ARN' (str)\n"
     ]
    }
   ],
   "source": [
    "%store ENDPOINT_ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(endpoint_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use the steps below to create a Comprehend endpoint using the AWS console.\n",
    "\n",
    "- Go to [Comprehend on AWS Console](https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#endpoints) and click on Endpoints in the left menu.\n",
    "- Click on \"Create endpoint\"\n",
    "- Give an Endpoint name; for Custom model type select Custom classification; for version select no version or the latest version of the model.\n",
    "- For Classifier model select from the drop down menu\n",
    "- For Inference Unit select 1\n",
    "- Check \"Acknowledge\"\n",
    "- Click \"Create endpoint\"\n",
    "\n",
    "[It may take ~15 minutes](https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#endpoints) for the endpoint to get created. The code cell below checks the creation status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "ep_arn = endpoint_response[\"EndpointArn\"]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_endpoint_resp = comprehend.describe_endpoint(\n",
    "        EndpointArn=ep_arn\n",
    "    )\n",
    "    status = describe_endpoint_resp[\"EndpointProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier: {status}\")\n",
    "    \n",
    "    if status == \"IN_SERVICE\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 3 (_optional_): Classify Documents using the real-time endpoint <a id=\"step3\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>‚ö†Ô∏è Note:</b> Execute this section only if you have created a real-time endpoint with the Amazon Comprehend custom classifier model.\n",
    "</div>\n",
    "\n",
    "Once the endpoint has been created, we will use a mix of documents under the `/samples/mixedbag/` directory and try to classify them to bank statement, invoice, and receipt documents respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./dataset/document_samples/\"\n",
    "files = []\n",
    "\n",
    "for file in os.listdir(root):\n",
    "    if not file.startswith('.'):\n",
    "        files.append(f'./dataset/document_samples/{file}')\n",
    "\n",
    "files_df = pd.DataFrame(files, columns=[\"Document\"])\n",
    "files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view one of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files_df.sample().iloc[0]['Document']\n",
    "display(Image(filename=file, width=400, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Text from the sample test documents using Textract. We will first convert the documents to ByteArray and then use Textract `detect_document_text` API to extract the text from the sample documents. We will create a utility function that reads each document and converts it into ByteArray for us to use with Textract. Once we extract text using Textract we will call Amazon Comprehend on each of them to classify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the text from all the sample documents in the list\n",
    "\n",
    "We will create yet another small utility function that receives the document bytearray, extracts text from the document with Textract and returns the extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(doc):\n",
    "    response = call_textract(input_document=doc)\n",
    "    page_string = get_string(textract_json=response, output_type=[Textract_Pretty_Print.LINES])\n",
    "    return page_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df['DocText'] = files_df.apply(lambda row : extract_text(row['Document']), axis = 1)\n",
    "files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the extracted text in the dataframe for each of our document, the next step is to use the Amazon Comprehend real-time endpoint to classify them. We will create a small utility function that does the classification using the endpoint and returns the document type. Note that Comprehend will return all the classes of documents with a confidence score linked to each class in an array of key-value pairs (Name-Score), we will pick only the document class with the highest confidence score from the endpoint's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "def classify_doc(document):\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(f'{current_time} : Processing')\n",
    "    time.sleep(10)                 #to avoid Comprehend API throttling\n",
    "    try:\n",
    "        response = comprehend.classify_document(\n",
    "            Text= document,\n",
    "            EndpointArn=ENDPOINT_ARN\n",
    "        )\n",
    "        print(response)\n",
    "        response_df = pd.DataFrame(response['Classes'])\n",
    "        result = response_df.iloc[response_df['Score'].idxmax()] # choose the class with highest score        \n",
    "        return result.Name                                       # return the corresponding class name\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now run the inference on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "files_df['DocType'] = files_df.apply(lambda row : classify_doc(row['DocText']), axis = 1)\n",
    "files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now classified all these documents into their respective classes. Let's review to check if the classifier did a correct job of classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = files_df.iloc[0]['Document']\n",
    "document_type = files_df.iloc[0]['DocType']\n",
    "\n",
    "display(HTML(f'<h2>Document Category: \"<u style=\"color:#00E676;\">{document_type}</u>\"</h2>'))\n",
    "display(Image(filename=document, width=400, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "As we can see from above, our classifier was able to correctly classify the test document!\n",
    "\n",
    "In this notebook we learned how to train an Amazon Comprehend custom classifier using our pre-prepared dataset, that was constructed from sample documents by extracting the text from the documents using Amazon Textract and labeling the data into a CSV file format. We then trained an Amazon Comprehend custom classifier with the extracted text and created an Amazon Comprehend Classifier real time endpoint to performe classification of documents.\n",
    "\n",
    "We also learned how to import an existing trained model from a different account and run asynchronous inference with the imported model without having to create a real-time endpoint.\n",
    "\n",
    "In the next notebook we will look at a few methods to perfrom extraction of key insights from our documents using Amazon Textract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
